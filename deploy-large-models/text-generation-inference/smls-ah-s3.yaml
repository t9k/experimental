apiVersion: tensorstack.dev/v1beta1
kind: SimpleMLService
metadata:
  name: huggingface-text-generation-inference
spec:
  replicas: 1
  scheduler:
    t9kScheduler:
      queue: default
  storage:
    s3:
      secretName: llama-7b
      uri: s3://e5522e58-42fa-4190-83de-27aea05bf60b/main/
  service:
    type: ClusterIP
    ports:
    - targetPort: 80
      port: 8080
  custom:
    spec:
      containers:
      - name: server
        image: ghcr.io/huggingface/text-generation-inference:0.8
        args:
        - --model-id=/var/lib/t9k/model
        - --num-shard=1
        ports:
        - containerPort: 80
        resources:
          limits:
            cpu: 4
            memory: 64Gi
            nvidia.com/gpu: 1
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
